diff -urN elpa_lib-201305.orig/CMakeLists.txt elpa_lib-201305/CMakeLists.txt
--- elpa_lib-201305.orig/CMakeLists.txt	1970-01-01 09:00:00.000000000 +0900
+++ elpa_lib-201305/CMakeLists.txt	2013-10-01 15:58:22.000000000 +0900
@@ -0,0 +1,20 @@
+cmake_minimum_required(VERSION 2.8)
+project(ELPA C Fortran)
+option(BUILD_SHARED_LIBS "Build shared libraries" ON)
+
+# RPATH setting
+if(APPLE)
+  set(CMAKE_INSTALL_NAME_DIR "${CMAKE_INSTALL_PREFIX}/lib")
+else(APPLE)
+  set(CMAKE_INSTALL_RPATH "${CMAKE_INSTALL_PREFIX}/lib")
+  set(CMAKE_SKIP_BUILD_RPATH FALSE)
+  set(CMAKE_BUILD_WITH_INSTALL_RPATH FALSE)
+  set(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)
+endif(APPLE)
+
+find_package(MPI)
+set(CMAKE_EXE_LINKER_FLAGS ${MPI_Fortran_LIBRARIES})
+
+add_subdirectory(ELPA_2011.12)
+add_subdirectory(ELPA_2013.02.BETA)
+add_subdirectory(ELPA_development_version)
diff -urN elpa_lib-201305.orig/ELPA_2011.12/CMakeLists.txt elpa_lib-201305/ELPA_2011.12/CMakeLists.txt
--- elpa_lib-201305.orig/ELPA_2011.12/CMakeLists.txt	1970-01-01 09:00:00.000000000 +0900
+++ elpa_lib-201305/ELPA_2011.12/CMakeLists.txt	2013-10-01 15:57:54.000000000 +0900
@@ -0,0 +1,11 @@
+set(SOURCES src/elpa1.f90 src/elpa2.f90 src/elpa2_kernels.f90 src/elpa1_wrap.f)
+add_library(elpa-2011.12 ${SOURCES})
+target_link_libraries(elpa-2011.12 ${SCALAPACK_LIB})
+install(TARGETS elpa-2011.12 ARCHIVE DESTINATION lib LIBRARY DESTINATION lib RUNTIME DESTINATION bin)
+
+set(TARGETS read_real read_real_gen test_complex test_complex2 test_complex_gen test_real test_real2 test_real_gen)
+foreach(t ${TARGETS})
+  add_executable(${t}-2011.12 test/${t}.f90)
+  set_target_properties(${t}-2011.12 PROPERTIES OUTPUT_NAME ${t})
+  target_link_libraries(${t}-2011.12 elpa-2011.12)
+endforeach(t ${TARGETS})
diff -urN elpa_lib-201305.orig/ELPA_2011.12/src/elpa1_wrap.f elpa_lib-201305/ELPA_2011.12/src/elpa1_wrap.f
--- elpa_lib-201305.orig/ELPA_2011.12/src/elpa1_wrap.f	1970-01-01 09:00:00.000000000 +0900
+++ elpa_lib-201305/ELPA_2011.12/src/elpa1_wrap.f	2013-10-01 15:57:54.000000000 +0900
@@ -0,0 +1,39 @@
+      subroutine get_elpa_row_col_comms_wrap(mpi_comm_global, 
+     &my_prow, my_pcol, mpi_comm_rows, mpi_comm_cols)
+
+      use ELPA1, only : get_elpa_row_col_comms
+
+      implicit none
+
+      include 'mpif.h'
+
+      integer, intent(in)  :: mpi_comm_global, my_prow, my_pcol
+      integer, intent(out) :: mpi_comm_rows, mpi_comm_cols
+
+      call get_elpa_row_col_comms(mpi_comm_global, my_prow, my_pcol,
+     &                          mpi_comm_rows, mpi_comm_cols)
+
+      return
+      end subroutine            ! get_elpa_row_col_comms_wrapper
+
+
+
+      subroutine solve_evp_real_wrap(na, nev, a, lda, ev, q, ldq, nblk,
+     &mpi_comm_rows, mpi_comm_cols)
+
+      use ELPA1, only : solve_evp_real
+
+      implicit none
+
+      include 'mpif.h'
+
+      integer, intent(in) :: na, nev, lda, ldq, nblk,
+     &mpi_comm_rows, mpi_comm_cols
+      real*8 :: a(lda,*), ev(na), q(ldq,*)
+
+      call solve_evp_real(na, nev, a, lda, ev, q, ldq, nblk,
+     &mpi_comm_rows, mpi_comm_cols)
+
+      return
+      end subroutine            ! solve_evp_real
+
diff -urN elpa_lib-201305.orig/ELPA_2011.12/src/elpa2.f90 elpa_lib-201305/ELPA_2011.12/src/elpa2.f90
--- elpa_lib-201305.orig/ELPA_2011.12/src/elpa2.f90	2013-05-21 14:44:19.000000000 +0900
+++ elpa_lib-201305/ELPA_2011.12/src/elpa2.f90	2013-10-01 15:50:08.000000000 +0900
@@ -2411,7 +2411,7 @@
       local_size = limits(my_prow+1) - limits(my_prow)
       if(mod(n-1,np_cols) == my_pcol .and. local_size>0 .and. nx>1) then
          num_chunks  = num_chunks+1
-         call mpi_irecv(hh_trans_complex(1,num_hh_vecs+1), nb*local_size, MPI_COMPLEX16, nt, &
+         call mpi_irecv(hh_trans_complex(1,num_hh_vecs+1), nb*local_size, MPI_DOUBLE_COMPLEX, nt, &
                         10+n-block_limits(nt), mpi_comm, ireq_hhr(num_chunks), mpierr)
          num_hh_vecs = num_hh_vecs + local_size
       endif
@@ -2458,7 +2458,7 @@
       ! send first column to previous PE
       ! Only the PE owning the diagonal does that (sending 1 element of the subdiagonal block also)
       ab_s(1:nb+1) = ab(1:nb+1,na_s-n_off)
-      call mpi_isend(ab_s,nb+1,MPI_COMPLEX16,my_pe-1,1,mpi_comm,ireq_ab,mpierr)
+      call mpi_isend(ab_s,nb+1,MPI_DOUBLE_COMPLEX,my_pe-1,1,mpi_comm,ireq_ab,mpierr)
    endif
 
    do istep=1,na-1
@@ -2486,7 +2486,7 @@
       else
          if(na>na_s) then
             ! Receive Householder vector from previous task, from PE owning subdiagonal
-            call mpi_recv(hv,nb,MPI_COMPLEX16,my_pe-1,2,mpi_comm,MPI_STATUS_IGNORE,mpierr)
+            call mpi_recv(hv,nb,MPI_DOUBLE_COMPLEX,my_pe-1,2,mpi_comm,MPI_STATUS_IGNORE,mpierr)
             tau = hv(1)
             hv(1) = 1.
          endif
@@ -2519,7 +2519,7 @@
             ! Copy vectors into send buffer
             hh_send(:,1:hh_cnt(iblk),iblk) = hh_gath(:,1:hh_cnt(iblk),iblk)
             ! Send to destination
-            call mpi_isend(hh_send(1,1,iblk), nb*hh_cnt(iblk), MPI_COMPLEX16, &
+            call mpi_isend(hh_send(1,1,iblk), nb*hh_cnt(iblk), MPI_DOUBLE_COMPLEX, &
                            global_id(hh_dst(iblk),mod(iblk+block_limits(my_pe)-1,np_cols)), &
                            10+iblk, mpi_comm, ireq_hhs(iblk), mpierr)
             ! Reset counter and increase destination row
@@ -2552,7 +2552,7 @@
             if(nr>0) call ZGEMV('N',nr,nb-1,tau,ab(nb+1,ns),2*nb-1,hv,1,(0.d0,0.d0),hs,1)
 
             ! ... then request last column ...
-            call mpi_recv(ab(1,ne),nb+1,MPI_COMPLEX16,my_pe+1,1,mpi_comm,MPI_STATUS_IGNORE,mpierr)
+            call mpi_recv(ab(1,ne),nb+1,MPI_DOUBLE_COMPLEX,my_pe+1,1,mpi_comm,MPI_STATUS_IGNORE,mpierr)
 
             ! ... and complete the result
             hs(1:nr) = hs(1:nr) + ab(2:nr+1,ne)*tau*hv(nb)
@@ -2593,7 +2593,7 @@
                call mpi_wait(ireq_hv,MPI_STATUS_IGNORE,mpierr)
                hv_s(1) = tau_new
                hv_s(2:) = hv_new(2:)
-               call mpi_isend(hv_s,nb,MPI_COMPLEX16,my_pe+1,2,mpi_comm,ireq_hv,mpierr)
+               call mpi_isend(hv_s,nb,MPI_DOUBLE_COMPLEX,my_pe+1,2,mpi_comm,ireq_hv,mpierr)
             endif
 
          endif
@@ -2614,7 +2614,7 @@
 
             call mpi_wait(ireq_ab,MPI_STATUS_IGNORE,mpierr)
             ab_s(1:nb+1) = ab(1:nb+1,ns)
-            call mpi_isend(ab_s,nb+1,MPI_COMPLEX16,my_pe-1,1,mpi_comm,ireq_ab,mpierr)
+            call mpi_isend(ab_s,nb+1,MPI_DOUBLE_COMPLEX,my_pe-1,1,mpi_comm,ireq_ab,mpierr)
 
             ! ... and calculate remaining columns with rank-2 update
             if(nc>1) call ZHER2('L',nc-1,(-1.d0,0.d0),hd(2),1,hv(2),1,ab(1,ns+1),2*nb-1)
@@ -2804,7 +2804,7 @@
             do i=limits(ip)+1,limits(ip+1)
                 src = mod((i-1)/nblk, np_rows)
                 if(src < my_prow) then
-                    call MPI_Recv(row, l_nev, MPI_COMPLEX16, src, 0, mpi_comm_rows, MPI_STATUS_IGNORE, mpierr)
+                    call MPI_Recv(row, l_nev, MPI_DOUBLE_COMPLEX, src, 0, mpi_comm_rows, MPI_STATUS_IGNORE, mpierr)
                     call unpack_row(row,i-limits(ip))
                 elseif(src==my_prow) then
                     src_offset = src_offset+1
@@ -2819,7 +2819,7 @@
                 if(mod((i-1)/nblk, np_rows) == my_prow) then
                     src_offset = src_offset+1
                     row(:) = q(src_offset, 1:l_nev)
-                    call MPI_Send(row, l_nev, MPI_COMPLEX16, dst, 0, mpi_comm_rows, mpierr)
+                    call MPI_Send(row, l_nev, MPI_DOUBLE_COMPLEX, dst, 0, mpi_comm_rows, mpierr)
                 endif
               enddo
             enddo
@@ -2831,14 +2831,14 @@
                 if(src == my_prow) then
                     src_offset = src_offset+1
                     row(:) = q(src_offset, 1:l_nev)
-                    call MPI_Send(row, l_nev, MPI_COMPLEX16, ip, 0, mpi_comm_rows, mpierr)
+                    call MPI_Send(row, l_nev, MPI_DOUBLE_COMPLEX, ip, 0, mpi_comm_rows, mpierr)
                 endif
             enddo
             ! Receive all rows from PE ip
             do i=limits(my_prow)+1,limits(my_prow+1)
                 src = mod((i-1)/nblk, np_rows)
                 if(src == ip) then
-                    call MPI_Recv(row, l_nev, MPI_COMPLEX16, src, 0, mpi_comm_rows, MPI_STATUS_IGNORE, mpierr)
+                    call MPI_Recv(row, l_nev, MPI_DOUBLE_COMPLEX, src, 0, mpi_comm_rows, MPI_STATUS_IGNORE, mpierr)
                     call unpack_row(row,i-limits(my_prow))
                 endif
             enddo
@@ -2862,7 +2862,7 @@
 
     if(my_prow > 0 .and. l_nev>0) then ! note: row 0 always sends
         do j = 1, min(num_result_buffers, num_result_blocks)
-            call MPI_Irecv(result_buffer(1,1,j), l_nev*nblk, MPI_COMPLEX16, 0, result_recv_tag, &
+            call MPI_Irecv(result_buffer(1,1,j), l_nev*nblk, MPI_DOUBLE_COMPLEX, 0, result_recv_tag, &
                            mpi_comm_rows, result_recv_request(j), mpierr)
         enddo
     endif
@@ -2934,7 +2934,7 @@
 
         if(sweep==0 .and. current_n_end < current_n .and. l_nev > 0) then
             do i = 1, stripe_count
-                call MPI_Irecv(bottom_border_recv_buffer(1,1,i), nbw*stripe_width, MPI_COMPLEX16, my_prow+1, bottom_recv_tag, &
+                call MPI_Irecv(bottom_border_recv_buffer(1,1,i), nbw*stripe_width, MPI_DOUBLE_COMPLEX, my_prow+1, bottom_recv_tag, &
                            mpi_comm_rows, bottom_recv_request(i), mpierr)
             enddo
         endif
@@ -2944,7 +2944,7 @@
                 bcast_buffer(:,1:current_local_n) = hh_trans_complex(:,current_tv_off+1:current_tv_off+current_local_n)
                 current_tv_off = current_tv_off + current_local_n
             endif
-            call mpi_bcast(bcast_buffer, nbw*current_local_n, MPI_COMPLEX16, mod(sweep,np_cols), mpi_comm_cols, mpierr)
+            call mpi_bcast(bcast_buffer, nbw*current_local_n, MPI_DOUBLE_COMPLEX, mod(sweep,np_cols), mpi_comm_cols, mpierr)
         else
             ! for current_local_n == 1 the one and only HH vector is 0 and not stored in hh_trans_complex
             bcast_buffer(:,1) = 0
@@ -2962,7 +2962,7 @@
                 n_off = current_local_n+a_off
                 a(:,n_off+1:n_off+nbw,i) = bottom_border_recv_buffer(:,1:nbw,i)
                 if(next_n_end < next_n) then
-                    call MPI_Irecv(bottom_border_recv_buffer(1,1,i), nbw*stripe_width, MPI_COMPLEX16, my_prow+1, bottom_recv_tag, &
+                    call MPI_Irecv(bottom_border_recv_buffer(1,1,i), nbw*stripe_width, MPI_DOUBLE_COMPLEX, my_prow+1, bottom_recv_tag, &
                                    mpi_comm_rows, bottom_recv_request(i), mpierr)
                 endif
             endif
@@ -2983,7 +2983,7 @@
                 if(bottom_msg_length>0) then
                     n_off = current_local_n+nbw-bottom_msg_length+a_off
                     bottom_border_send_buffer(:,1:bottom_msg_length,i) = a(:,n_off+1:n_off+bottom_msg_length,i)
-                    call MPI_Isend(bottom_border_send_buffer(1,1,i), bottom_msg_length*stripe_width, MPI_COMPLEX16, my_prow+1, &
+                    call MPI_Isend(bottom_border_send_buffer(1,1,i), bottom_msg_length*stripe_width, MPI_DOUBLE_COMPLEX, my_prow+1, &
                                    top_recv_tag, mpi_comm_rows, bottom_send_request(i), mpierr)
                 endif
 
@@ -2997,7 +2997,7 @@
                 if(bottom_msg_length > 0) then
                     n_off = current_local_n+nbw-bottom_msg_length+a_off
                     bottom_border_send_buffer(:,1:bottom_msg_length,i) = a(:,n_off+1:n_off+bottom_msg_length,i)
-                    call MPI_Isend(bottom_border_send_buffer(1,1,i), bottom_msg_length*stripe_width, MPI_COMPLEX16, my_prow+1, &
+                    call MPI_Isend(bottom_border_send_buffer(1,1,i), bottom_msg_length*stripe_width, MPI_DOUBLE_COMPLEX, my_prow+1, &
                                    top_recv_tag, mpi_comm_rows, bottom_send_request(i), mpierr)
                 endif
 
@@ -3016,7 +3016,7 @@
 
             if(next_top_msg_length > 0) then
                 !request top_border data
-                call MPI_Irecv(top_border_recv_buffer(1,1,i), next_top_msg_length*stripe_width, MPI_COMPLEX16, my_prow-1, &
+                call MPI_Irecv(top_border_recv_buffer(1,1,i), next_top_msg_length*stripe_width, MPI_DOUBLE_COMPLEX, my_prow-1, &
                                top_recv_tag, mpi_comm_rows, top_recv_request(i), mpierr)
             endif
 
@@ -3024,7 +3024,7 @@
             if(my_prow > 0) then
                 call MPI_Wait(top_send_request(i), MPI_STATUS_IGNORE, mpierr)
                 top_border_send_buffer(:,1:nbw,i) = a(:,a_off+1:a_off+nbw,i)
-                call MPI_Isend(top_border_send_buffer(1,1,i), nbw*stripe_width, MPI_COMPLEX16, my_prow-1, bottom_recv_tag, &
+                call MPI_Isend(top_border_send_buffer(1,1,i), nbw*stripe_width, MPI_DOUBLE_COMPLEX, my_prow-1, bottom_recv_tag, &
                                mpi_comm_rows, top_send_request(i), mpierr)
             endif
 
@@ -3074,7 +3074,7 @@
                     do i = 1, nblk
                         call pack_row(result_buffer(:,i,nbuf),j*nblk+i+a_off)
                     enddo
-                    call MPI_Isend(result_buffer(1,1,nbuf), l_nev*nblk, MPI_COMPLEX16, dst, &
+                    call MPI_Isend(result_buffer(1,1,nbuf), l_nev*nblk, MPI_DOUBLE_COMPLEX, dst, &
                                    result_recv_tag, mpi_comm_rows, result_send_request(nbuf), mpierr)
                 endif
             enddo
@@ -3106,7 +3106,7 @@
 
                 ! Queue result buffer again if there are outstanding blocks left
                 if(j+num_result_buffers < num_result_blocks) &
-                    call MPI_Irecv(result_buffer(1,1,nbuf), l_nev*nblk, MPI_COMPLEX16, 0, result_recv_tag, &
+                    call MPI_Irecv(result_buffer(1,1,nbuf), l_nev*nblk, MPI_DOUBLE_COMPLEX, 0, result_recv_tag, &
                                    mpi_comm_rows, result_recv_request(nbuf), mpierr)
 
             enddo
@@ -3362,7 +3362,7 @@
    if(l_real) then
       call MPI_Alltoallv(r_sbuf,ncnt_s,nstart_s,MPI_REAL8,r_rbuf,ncnt_r,nstart_r,MPI_REAL8,mpi_comm,mpierr)
    else
-      call MPI_Alltoallv(c_sbuf,ncnt_s,nstart_s,MPI_COMPLEX16,c_rbuf,ncnt_r,nstart_r,MPI_COMPLEX16,mpi_comm,mpierr)
+      call MPI_Alltoallv(c_sbuf,ncnt_s,nstart_s,MPI_DOUBLE_COMPLEX,c_rbuf,ncnt_r,nstart_r,MPI_DOUBLE_COMPLEX,mpi_comm,mpierr)
    endif
 
    ! set band from receive buffer
diff -urN elpa_lib-201305.orig/ELPA_2013.02.BETA/CMakeLists.txt elpa_lib-201305/ELPA_2013.02.BETA/CMakeLists.txt
--- elpa_lib-201305.orig/ELPA_2013.02.BETA/CMakeLists.txt	1970-01-01 09:00:00.000000000 +0900
+++ elpa_lib-201305/ELPA_2013.02.BETA/CMakeLists.txt	2013-10-01 15:57:54.000000000 +0900
@@ -0,0 +1,11 @@
+set(SOURCES src/elpa1.f90 src/elpa2.f90 src/elpa2_kernels/elpa2_kernels_complex.f90 src/elpa2_kernels/elpa2_kernels_real.f90 src/elpa1_wrap.f)
+add_library(elpa-2013.02.beta ${SOURCES})
+target_link_libraries(elpa-2013.02.beta ${SCALAPACK_LIB})
+install(TARGETS elpa-2013.02.beta ARCHIVE DESTINATION lib LIBRARY DESTINATION lib RUNTIME DESTINATION bin)
+
+set(TARGETS read_real read_real_gen test_complex test_complex2 test_complex_gen test_real test_real2 test_real_gen)
+foreach(t ${TARGETS})
+  add_executable(${t}-2013.02.beta test/${t}.f90)
+  set_target_properties(${t}-2013.02.beta PROPERTIES OUTPUT_NAME ${t})
+  target_link_libraries(${t}-2013.02.beta elpa-2013.02.beta)
+endforeach(t ${TARGETS})
diff -urN elpa_lib-201305.orig/ELPA_2013.02.BETA/src/elpa1_wrap.f elpa_lib-201305/ELPA_2013.02.BETA/src/elpa1_wrap.f
--- elpa_lib-201305.orig/ELPA_2013.02.BETA/src/elpa1_wrap.f	1970-01-01 09:00:00.000000000 +0900
+++ elpa_lib-201305/ELPA_2013.02.BETA/src/elpa1_wrap.f	2013-10-01 15:57:54.000000000 +0900
@@ -0,0 +1,39 @@
+      subroutine get_elpa_row_col_comms_wrap(mpi_comm_global, 
+     &my_prow, my_pcol, mpi_comm_rows, mpi_comm_cols)
+
+      use ELPA1, only : get_elpa_row_col_comms
+
+      implicit none
+
+      include 'mpif.h'
+
+      integer, intent(in)  :: mpi_comm_global, my_prow, my_pcol
+      integer, intent(out) :: mpi_comm_rows, mpi_comm_cols
+
+      call get_elpa_row_col_comms(mpi_comm_global, my_prow, my_pcol,
+     &                          mpi_comm_rows, mpi_comm_cols)
+
+      return
+      end subroutine            ! get_elpa_row_col_comms_wrapper
+
+
+
+      subroutine solve_evp_real_wrap(na, nev, a, lda, ev, q, ldq, nblk,
+     &mpi_comm_rows, mpi_comm_cols)
+
+      use ELPA1, only : solve_evp_real
+
+      implicit none
+
+      include 'mpif.h'
+
+      integer, intent(in) :: na, nev, lda, ldq, nblk,
+     &mpi_comm_rows, mpi_comm_cols
+      real*8 :: a(lda,*), ev(na), q(ldq,*)
+
+      call solve_evp_real(na, nev, a, lda, ev, q, ldq, nblk,
+     &mpi_comm_rows, mpi_comm_cols)
+
+      return
+      end subroutine            ! solve_evp_real
+
diff -urN elpa_lib-201305.orig/ELPA_2013.02.BETA/src/elpa2.f90 elpa_lib-201305/ELPA_2013.02.BETA/src/elpa2.f90
--- elpa_lib-201305.orig/ELPA_2013.02.BETA/src/elpa2.f90	2013-05-21 14:44:19.000000000 +0900
+++ elpa_lib-201305/ELPA_2013.02.BETA/src/elpa2.f90	2013-10-01 15:56:48.000000000 +0900
@@ -2461,7 +2461,7 @@
       local_size = limits(my_prow+1) - limits(my_prow)
       if(mod(n-1,np_cols) == my_pcol .and. local_size>0 .and. nx>1) then
          num_chunks  = num_chunks+1
-         call mpi_irecv(hh_trans_complex(1,num_hh_vecs+1), nb*local_size, MPI_COMPLEX16, nt, &
+         call mpi_irecv(hh_trans_complex(1,num_hh_vecs+1), nb*local_size, MPI_DOUBLE_COMPLEX, nt, &
                         10+n-block_limits(nt), mpi_comm, ireq_hhr(num_chunks), mpierr)
          num_hh_vecs = num_hh_vecs + local_size
       endif
@@ -2508,7 +2508,7 @@
       ! send first column to previous PE
       ! Only the PE owning the diagonal does that (sending 1 element of the subdiagonal block also)
       ab_s(1:nb+1) = ab(1:nb+1,na_s-n_off)
-      call mpi_isend(ab_s,nb+1,MPI_COMPLEX16,my_pe-1,1,mpi_comm,ireq_ab,mpierr)
+      call mpi_isend(ab_s,nb+1,MPI_DOUBLE_COMPLEX,my_pe-1,1,mpi_comm,ireq_ab,mpierr)
    endif
 
    do istep=1,na-1
@@ -2536,7 +2536,7 @@
       else
          if(na>na_s) then
             ! Receive Householder vector from previous task, from PE owning subdiagonal
-            call mpi_recv(hv,nb,MPI_COMPLEX16,my_pe-1,2,mpi_comm,MPI_STATUS_IGNORE,mpierr)
+            call mpi_recv(hv,nb,MPI_DOUBLE_COMPLEX,my_pe-1,2,mpi_comm,MPI_STATUS_IGNORE,mpierr)
             tau = hv(1)
             hv(1) = 1.
          endif
@@ -2569,7 +2569,7 @@
             ! Copy vectors into send buffer
             hh_send(:,1:hh_cnt(iblk),iblk) = hh_gath(:,1:hh_cnt(iblk),iblk)
             ! Send to destination
-            call mpi_isend(hh_send(1,1,iblk), nb*hh_cnt(iblk), MPI_COMPLEX16, &
+            call mpi_isend(hh_send(1,1,iblk), nb*hh_cnt(iblk), MPI_DOUBLE_COMPLEX, &
                            global_id(hh_dst(iblk),mod(iblk+block_limits(my_pe)-1,np_cols)), &
                            10+iblk, mpi_comm, ireq_hhs(iblk), mpierr)
             ! Reset counter and increase destination row
@@ -2602,7 +2602,7 @@
             if(nr>0) call ZGEMV('N',nr,nb-1,tau,ab(nb+1,ns),2*nb-1,hv,1,(0.d0,0.d0),hs,1)
 
             ! ... then request last column ...
-            call mpi_recv(ab(1,ne),nb+1,MPI_COMPLEX16,my_pe+1,1,mpi_comm,MPI_STATUS_IGNORE,mpierr)
+            call mpi_recv(ab(1,ne),nb+1,MPI_DOUBLE_COMPLEX,my_pe+1,1,mpi_comm,MPI_STATUS_IGNORE,mpierr)
 
             ! ... and complete the result
             hs(1:nr) = hs(1:nr) + ab(2:nr+1,ne)*tau*hv(nb)
@@ -2643,7 +2643,7 @@
                call mpi_wait(ireq_hv,MPI_STATUS_IGNORE,mpierr)
                hv_s(1) = tau_new
                hv_s(2:) = hv_new(2:)
-               call mpi_isend(hv_s,nb,MPI_COMPLEX16,my_pe+1,2,mpi_comm,ireq_hv,mpierr)
+               call mpi_isend(hv_s,nb,MPI_DOUBLE_COMPLEX,my_pe+1,2,mpi_comm,ireq_hv,mpierr)
             endif
 
          endif
@@ -2664,7 +2664,7 @@
 
             call mpi_wait(ireq_ab,MPI_STATUS_IGNORE,mpierr)
             ab_s(1:nb+1) = ab(1:nb+1,ns)
-            call mpi_isend(ab_s,nb+1,MPI_COMPLEX16,my_pe-1,1,mpi_comm,ireq_ab,mpierr)
+            call mpi_isend(ab_s,nb+1,MPI_DOUBLE_COMPLEX,my_pe-1,1,mpi_comm,ireq_ab,mpierr)
 
             ! ... and calculate remaining columns with rank-2 update
             if(nc>1) call ZHER2('L',nc-1,(-1.d0,0.d0),hd(2),1,hv(2),1,ab(1,ns+1),2*nb-1)
@@ -2855,7 +2855,7 @@
             do i=limits(ip)+1,limits(ip+1)
                 src = mod((i-1)/nblk, np_rows)
                 if(src < my_prow) then
-                    call MPI_Recv(row, l_nev, MPI_COMPLEX16, src, 0, mpi_comm_rows, MPI_STATUS_IGNORE, mpierr)
+                    call MPI_Recv(row, l_nev, MPI_DOUBLE_COMPLEX, src, 0, mpi_comm_rows, MPI_STATUS_IGNORE, mpierr)
                     call unpack_row(row,i-limits(ip))
                 elseif(src==my_prow) then
                     src_offset = src_offset+1
@@ -2870,7 +2870,7 @@
                 if(mod((i-1)/nblk, np_rows) == my_prow) then
                     src_offset = src_offset+1
                     row(:) = q(src_offset, 1:l_nev)
-                    call MPI_Send(row, l_nev, MPI_COMPLEX16, dst, 0, mpi_comm_rows, mpierr)
+                    call MPI_Send(row, l_nev, MPI_DOUBLE_COMPLEX, dst, 0, mpi_comm_rows, mpierr)
                 endif
               enddo
             enddo
@@ -2882,14 +2882,14 @@
                 if(src == my_prow) then
                     src_offset = src_offset+1
                     row(:) = q(src_offset, 1:l_nev)
-                    call MPI_Send(row, l_nev, MPI_COMPLEX16, ip, 0, mpi_comm_rows, mpierr)
+                    call MPI_Send(row, l_nev, MPI_DOUBLE_COMPLEX, ip, 0, mpi_comm_rows, mpierr)
                 endif
             enddo
             ! Receive all rows from PE ip
             do i=limits(my_prow)+1,limits(my_prow+1)
                 src = mod((i-1)/nblk, np_rows)
                 if(src == ip) then
-                    call MPI_Recv(row, l_nev, MPI_COMPLEX16, src, 0, mpi_comm_rows, MPI_STATUS_IGNORE, mpierr)
+                    call MPI_Recv(row, l_nev, MPI_DOUBLE_COMPLEX, src, 0, mpi_comm_rows, MPI_STATUS_IGNORE, mpierr)
                     call unpack_row(row,i-limits(my_prow))
                 endif
             enddo
@@ -2913,7 +2913,7 @@
 
     if(my_prow > 0 .and. l_nev>0) then ! note: row 0 always sends
         do j = 1, min(num_result_buffers, num_result_blocks)
-            call MPI_Irecv(result_buffer(1,1,j), l_nev*nblk, MPI_COMPLEX16, 0, result_recv_tag, &
+            call MPI_Irecv(result_buffer(1,1,j), l_nev*nblk, MPI_DOUBLE_COMPLEX, 0, result_recv_tag, &
                            mpi_comm_rows, result_recv_request(j), mpierr)
         enddo
     endif
@@ -2985,7 +2985,7 @@
 
         if(sweep==0 .and. current_n_end < current_n .and. l_nev > 0) then
             do i = 1, stripe_count
-                call MPI_Irecv(bottom_border_recv_buffer(1,1,i), nbw*stripe_width, MPI_COMPLEX16, my_prow+1, bottom_recv_tag, &
+                call MPI_Irecv(bottom_border_recv_buffer(1,1,i), nbw*stripe_width, MPI_DOUBLE_COMPLEX, my_prow+1, bottom_recv_tag, &
                            mpi_comm_rows, bottom_recv_request(i), mpierr)
             enddo
         endif
@@ -2995,7 +2995,7 @@
                 bcast_buffer(:,1:current_local_n) = hh_trans_complex(:,current_tv_off+1:current_tv_off+current_local_n)
                 current_tv_off = current_tv_off + current_local_n
             endif
-            call mpi_bcast(bcast_buffer, nbw*current_local_n, MPI_COMPLEX16, mod(sweep,np_cols), mpi_comm_cols, mpierr)
+            call mpi_bcast(bcast_buffer, nbw*current_local_n, MPI_DOUBLE_COMPLEX, mod(sweep,np_cols), mpi_comm_cols, mpierr)
         else
             ! for current_local_n == 1 the one and only HH vector is 0 and not stored in hh_trans_complex
             bcast_buffer(:,1) = 0
@@ -3013,7 +3013,7 @@
                 n_off = current_local_n+a_off
                 a(:,n_off+1:n_off+nbw,i) = bottom_border_recv_buffer(:,1:nbw,i)
                 if(next_n_end < next_n) then
-                    call MPI_Irecv(bottom_border_recv_buffer(1,1,i), nbw*stripe_width, MPI_COMPLEX16, my_prow+1, bottom_recv_tag, &
+                    call MPI_Irecv(bottom_border_recv_buffer(1,1,i), nbw*stripe_width, MPI_DOUBLE_COMPLEX, my_prow+1, bottom_recv_tag, &
                                    mpi_comm_rows, bottom_recv_request(i), mpierr)
                 endif
             endif
@@ -3034,7 +3034,7 @@
                 if(bottom_msg_length>0) then
                     n_off = current_local_n+nbw-bottom_msg_length+a_off
                     bottom_border_send_buffer(:,1:bottom_msg_length,i) = a(:,n_off+1:n_off+bottom_msg_length,i)
-                    call MPI_Isend(bottom_border_send_buffer(1,1,i), bottom_msg_length*stripe_width, MPI_COMPLEX16, my_prow+1, &
+                    call MPI_Isend(bottom_border_send_buffer(1,1,i), bottom_msg_length*stripe_width, MPI_DOUBLE_COMPLEX, my_prow+1, &
                                    top_recv_tag, mpi_comm_rows, bottom_send_request(i), mpierr)
                 endif
 
@@ -3048,7 +3048,7 @@
                 if(bottom_msg_length > 0) then
                     n_off = current_local_n+nbw-bottom_msg_length+a_off
                     bottom_border_send_buffer(:,1:bottom_msg_length,i) = a(:,n_off+1:n_off+bottom_msg_length,i)
-                    call MPI_Isend(bottom_border_send_buffer(1,1,i), bottom_msg_length*stripe_width, MPI_COMPLEX16, my_prow+1, &
+                    call MPI_Isend(bottom_border_send_buffer(1,1,i), bottom_msg_length*stripe_width, MPI_DOUBLE_COMPLEX, my_prow+1, &
                                    top_recv_tag, mpi_comm_rows, bottom_send_request(i), mpierr)
                 endif
 
@@ -3067,7 +3067,7 @@
 
             if(next_top_msg_length > 0) then
                 !request top_border data
-                call MPI_Irecv(top_border_recv_buffer(1,1,i), next_top_msg_length*stripe_width, MPI_COMPLEX16, my_prow-1, &
+                call MPI_Irecv(top_border_recv_buffer(1,1,i), next_top_msg_length*stripe_width, MPI_DOUBLE_COMPLEX, my_prow-1, &
                                top_recv_tag, mpi_comm_rows, top_recv_request(i), mpierr)
             endif
 
@@ -3075,7 +3075,7 @@
             if(my_prow > 0) then
                 call MPI_Wait(top_send_request(i), MPI_STATUS_IGNORE, mpierr)
                 top_border_send_buffer(:,1:nbw,i) = a(:,a_off+1:a_off+nbw,i)
-                call MPI_Isend(top_border_send_buffer(1,1,i), nbw*stripe_width, MPI_COMPLEX16, my_prow-1, bottom_recv_tag, &
+                call MPI_Isend(top_border_send_buffer(1,1,i), nbw*stripe_width, MPI_DOUBLE_COMPLEX, my_prow-1, bottom_recv_tag, &
                                mpi_comm_rows, top_send_request(i), mpierr)
             endif
 
@@ -3125,7 +3125,7 @@
                     do i = 1, nblk
                         call pack_row(result_buffer(:,i,nbuf),j*nblk+i+a_off)
                     enddo
-                    call MPI_Isend(result_buffer(1,1,nbuf), l_nev*nblk, MPI_COMPLEX16, dst, &
+                    call MPI_Isend(result_buffer(1,1,nbuf), l_nev*nblk, MPI_DOUBLE_COMPLEX, dst, &
                                    result_recv_tag, mpi_comm_rows, result_send_request(nbuf), mpierr)
                 endif
             enddo
@@ -3157,7 +3157,7 @@
 
                 ! Queue result buffer again if there are outstanding blocks left
                 if(j+num_result_buffers < num_result_blocks) &
-                    call MPI_Irecv(result_buffer(1,1,nbuf), l_nev*nblk, MPI_COMPLEX16, 0, result_recv_tag, &
+                    call MPI_Irecv(result_buffer(1,1,nbuf), l_nev*nblk, MPI_DOUBLE_COMPLEX, 0, result_recv_tag, &
                                    mpi_comm_rows, result_recv_request(nbuf), mpierr)
 
             enddo
@@ -3433,7 +3433,7 @@
    if(l_real) then
       call MPI_Alltoallv(r_sbuf,ncnt_s,nstart_s,MPI_REAL8,r_rbuf,ncnt_r,nstart_r,MPI_REAL8,mpi_comm,mpierr)
    else
-      call MPI_Alltoallv(c_sbuf,ncnt_s,nstart_s,MPI_COMPLEX16,c_rbuf,ncnt_r,nstart_r,MPI_COMPLEX16,mpi_comm,mpierr)
+      call MPI_Alltoallv(c_sbuf,ncnt_s,nstart_s,MPI_DOUBLE_COMPLEX,c_rbuf,ncnt_r,nstart_r,MPI_DOUBLE_COMPLEX,mpi_comm,mpierr)
    endif
 
    ! set band from receive buffer
diff -urN elpa_lib-201305.orig/ELPA_2013.02.BETA/test/test_complex2.f90 elpa_lib-201305/ELPA_2013.02.BETA/test/test_complex2.f90
--- elpa_lib-201305.orig/ELPA_2013.02.BETA/test/test_complex2.f90	2013-05-21 14:44:19.000000000 +0900
+++ elpa_lib-201305/ELPA_2013.02.BETA/test/test_complex2.f90	2013-10-01 15:57:54.000000000 +0900
@@ -58,6 +58,7 @@
 
    implicit none
    include 'mpif.h'
+   integer :: iargc
 
    !-------------------------------------------------------------------------------
    ! Please set system size parameters below!
diff -urN elpa_lib-201305.orig/ELPA_2013.02.BETA/test/test_real2.f90 elpa_lib-201305/ELPA_2013.02.BETA/test/test_real2.f90
--- elpa_lib-201305.orig/ELPA_2013.02.BETA/test/test_real2.f90	2013-05-21 14:44:19.000000000 +0900
+++ elpa_lib-201305/ELPA_2013.02.BETA/test/test_real2.f90	2013-10-01 15:57:54.000000000 +0900
@@ -59,6 +59,7 @@
 
    implicit none
    include 'mpif.h'
+   integer :: iargc
 
    !-------------------------------------------------------------------------------
    ! Please set system size parameters below!
diff -urN elpa_lib-201305.orig/ELPA_development_version/CMakeLists.txt elpa_lib-201305/ELPA_development_version/CMakeLists.txt
--- elpa_lib-201305.orig/ELPA_development_version/CMakeLists.txt	1970-01-01 09:00:00.000000000 +0900
+++ elpa_lib-201305/ELPA_development_version/CMakeLists.txt	2013-10-01 15:57:54.000000000 +0900
@@ -0,0 +1,11 @@
+set(SOURCES src/elpa1.f90 src/elpa2.f90 src/elpa2_kernels/elpa2_kernels_complex.f90 src/elpa2_kernels/elpa2_kernels_real.f90 src/elpa_qr/elpa_pdgeqrf.f90 src/elpa_qr/elpa_pdlarfb.f90 src/elpa_qr/elpa_qrkernels.f90 src/elpa_qr/tum_utils.f90 src/elpa1_wrap.f)
+add_library(elpa-develop ${SOURCES})
+target_link_libraries(elpa-develop ${SCALAPACK_LIB})
+install(TARGETS elpa-develop ARCHIVE DESTINATION lib LIBRARY DESTINATION lib RUNTIME DESTINATION bin)
+
+set(TARGETS read_real read_real_gen test_complex test_complex2 test_complex_gen test_real test_real2 test_real_gen)
+foreach(t ${TARGETS})
+  add_executable(${t}-develop test/${t}.f90 test/read_test_parameters.f90)
+  set_target_properties(${t}-develop PROPERTIES OUTPUT_NAME ${t})
+  target_link_libraries(${t}-develop elpa-develop)
+endforeach(t ${TARGETS})
diff -urN elpa_lib-201305.orig/ELPA_development_version/src/elpa1_wrap.f elpa_lib-201305/ELPA_development_version/src/elpa1_wrap.f
--- elpa_lib-201305.orig/ELPA_development_version/src/elpa1_wrap.f	1970-01-01 09:00:00.000000000 +0900
+++ elpa_lib-201305/ELPA_development_version/src/elpa1_wrap.f	2013-10-01 15:57:54.000000000 +0900
@@ -0,0 +1,39 @@
+      subroutine get_elpa_row_col_comms_wrap(mpi_comm_global, 
+     &my_prow, my_pcol, mpi_comm_rows, mpi_comm_cols)
+
+      use ELPA1, only : get_elpa_row_col_comms
+
+      implicit none
+
+      include 'mpif.h'
+
+      integer, intent(in)  :: mpi_comm_global, my_prow, my_pcol
+      integer, intent(out) :: mpi_comm_rows, mpi_comm_cols
+
+      call get_elpa_row_col_comms(mpi_comm_global, my_prow, my_pcol,
+     &                          mpi_comm_rows, mpi_comm_cols)
+
+      return
+      end subroutine            ! get_elpa_row_col_comms_wrapper
+
+
+
+      subroutine solve_evp_real_wrap(na, nev, a, lda, ev, q, ldq, nblk,
+     &mpi_comm_rows, mpi_comm_cols)
+
+      use ELPA1, only : solve_evp_real
+
+      implicit none
+
+      include 'mpif.h'
+
+      integer, intent(in) :: na, nev, lda, ldq, nblk,
+     &mpi_comm_rows, mpi_comm_cols
+      real*8 :: a(lda,*), ev(na), q(ldq,*)
+
+      call solve_evp_real(na, nev, a, lda, ev, q, ldq, nblk,
+     &mpi_comm_rows, mpi_comm_cols)
+
+      return
+      end subroutine            ! solve_evp_real
+
diff -urN elpa_lib-201305.orig/ELPA_development_version/src/elpa2.f90 elpa_lib-201305/ELPA_development_version/src/elpa2.f90
--- elpa_lib-201305.orig/ELPA_development_version/src/elpa2.f90	2013-05-21 14:44:19.000000000 +0900
+++ elpa_lib-201305/ELPA_development_version/src/elpa2.f90	2013-10-01 15:59:18.000000000 +0900
@@ -2814,7 +2814,7 @@
       local_size = limits(my_prow+1) - limits(my_prow)
       if(mod(n-1,np_cols) == my_pcol .and. local_size>0 .and. nx>1) then
         num_chunks  = num_chunks+1
-        call mpi_irecv(hh_trans_complex(1,num_hh_vecs+1), nb*local_size, MPI_COMPLEX16, nt, &
+        call mpi_irecv(hh_trans_complex(1,num_hh_vecs+1), nb*local_size, MPI_DOUBLE_COMPLEX, nt, &
                        10+n-block_limits(nt), mpi_comm, ireq_hhr(num_chunks), mpierr)
         num_hh_vecs = num_hh_vecs + local_size
       endif
@@ -2879,7 +2879,7 @@
       ! send first column to previous PE
       ! Only the PE owning the diagonal does that (sending 1 element of the subdiagonal block also)
       ab_s(1:nb+1) = ab(1:nb+1,na_s-n_off)
-      call mpi_isend(ab_s,nb+1,MPI_COMPLEX16,my_pe-1,1,mpi_comm,ireq_ab,mpierr)
+      call mpi_isend(ab_s,nb+1,MPI_DOUBLE_COMPLEX,my_pe-1,1,mpi_comm,ireq_ab,mpierr)
     endif
 
     do istep=1,na-1-block_limits(my_pe)*nb
@@ -2907,7 +2907,7 @@
       else
         if(na>na_s) then
           ! Receive Householder vector from previous task, from PE owning subdiagonal
-          call mpi_recv(hv,nb,MPI_COMPLEX16,my_pe-1,2,mpi_comm,mpi_status,mpierr)
+          call mpi_recv(hv,nb,MPI_DOUBLE_COMPLEX,my_pe-1,2,mpi_comm,mpi_status,mpierr)
           tau = hv(1)
           hv(1) = 1.
         endif
@@ -3045,13 +3045,13 @@
             if(my_pe>0 .and. na_s <= na) then
               call mpi_wait(ireq_ab,mpi_status,mpierr)
               ab_s(1:nb+1) = ab(1:nb+1,na_s-n_off)
-              call mpi_isend(ab_s,nb+1,MPI_COMPLEX16,my_pe-1,1,mpi_comm,ireq_ab,mpierr)
+              call mpi_isend(ab_s,nb+1,MPI_DOUBLE_COMPLEX,my_pe-1,1,mpi_comm,ireq_ab,mpierr)
             endif
 
             ! Request last column from next PE
             ne = na_s + nblocks*nb - (max_threads-1) - 1
             if(istep>=max_threads .and. ne <= na) then
-              call mpi_recv(ab(1,ne-n_off),nb+1,MPI_COMPLEX16,my_pe+1,1,mpi_comm,mpi_status,mpierr)
+              call mpi_recv(ab(1,ne-n_off),nb+1,MPI_DOUBLE_COMPLEX,my_pe+1,1,mpi_comm,mpi_status,mpierr)
             endif
 
           else
@@ -3063,7 +3063,7 @@
               call mpi_wait(ireq_hv,mpi_status,mpierr)
               hv_s(1) = tau_t(max_threads)
               hv_s(2:) = hv_t(2:,max_threads)
-              call mpi_isend(hv_s,nb,MPI_COMPLEX16,my_pe+1,2,mpi_comm,ireq_hv,mpierr)
+              call mpi_isend(hv_s,nb,MPI_DOUBLE_COMPLEX,my_pe+1,2,mpi_comm,ireq_hv,mpierr)
             endif
 
             ! "Send" HH vector and TAU to next OpenMP thread
@@ -3118,7 +3118,7 @@
             if(nr>0) call ZGEMV('N',nr,nb-1,tau,ab(nb+1,ns),2*nb-1,hv,1,(0.d0,0.d0),hs,1)
 
             ! ... then request last column ...
-            call mpi_recv(ab(1,ne),nb+1,MPI_COMPLEX16,my_pe+1,1,mpi_comm,mpi_status,mpierr)
+            call mpi_recv(ab(1,ne),nb+1,MPI_DOUBLE_COMPLEX,my_pe+1,1,mpi_comm,mpi_status,mpierr)
 
             ! ... and complete the result
             hs(1:nr) = hs(1:nr) + ab(2:nr+1,ne)*tau*hv(nb)
@@ -3159,7 +3159,7 @@
               call mpi_wait(ireq_hv,mpi_status,mpierr)
               hv_s(1) = tau_new
               hv_s(2:) = hv_new(2:)
-              call mpi_isend(hv_s,nb,MPI_COMPLEX16,my_pe+1,2,mpi_comm,ireq_hv,mpierr)
+              call mpi_isend(hv_s,nb,MPI_DOUBLE_COMPLEX,my_pe+1,2,mpi_comm,ireq_hv,mpierr)
             endif
 
           endif
@@ -3180,7 +3180,7 @@
 
             call mpi_wait(ireq_ab,mpi_status,mpierr)
             ab_s(1:nb+1) = ab(1:nb+1,ns)
-            call mpi_isend(ab_s,nb+1,MPI_COMPLEX16,my_pe-1,1,mpi_comm,ireq_ab,mpierr)
+            call mpi_isend(ab_s,nb+1,MPI_DOUBLE_COMPLEX,my_pe-1,1,mpi_comm,ireq_ab,mpierr)
 
             ! ... and calculate remaining columns with rank-2 update
             if(nc>1) call ZHER2('L',nc-1,(-1.d0,0.d0),hd(2),1,hv(2),1,ab(1,ns+1),2*nb-1)
@@ -3227,7 +3227,7 @@
           ! Copy vectors into send buffer
           hh_send(:,1:hh_cnt(iblk),iblk) = hh_gath(:,1:hh_cnt(iblk),iblk)
           ! Send to destination
-          call mpi_isend(hh_send(1,1,iblk), nb*hh_cnt(iblk), MPI_COMPLEX16, &
+          call mpi_isend(hh_send(1,1,iblk), nb*hh_cnt(iblk), MPI_DOUBLE_COMPLEX, &
                          global_id(hh_dst(iblk),mod(iblk+block_limits(my_pe)-1,np_cols)), &
                          10+iblk, mpi_comm, ireq_hhs(iblk), mpierr)
           ! Reset counter and increase destination row
@@ -3412,7 +3412,7 @@
             do i=limits(ip)+1,limits(ip+1)
                 src = mod((i-1)/nblk, np_rows)
                 if(src < my_prow) then
-                    call MPI_Recv(row, l_nev, MPI_COMPLEX16, src, 0, mpi_comm_rows, mpi_status, mpierr)
+                    call MPI_Recv(row, l_nev, MPI_DOUBLE_COMPLEX, src, 0, mpi_comm_rows, mpi_status, mpierr)
 !$omp parallel do private(my_thread), schedule(static, 1)
                     do my_thread = 1, max_threads
                         call unpack_row(row,i-limits(ip),my_thread)
@@ -3433,7 +3433,7 @@
                 if(mod((i-1)/nblk, np_rows) == my_prow) then
                     src_offset = src_offset+1
                     row(:) = q(src_offset, 1:l_nev)
-                    call MPI_Send(row, l_nev, MPI_COMPLEX16, dst, 0, mpi_comm_rows, mpierr)
+                    call MPI_Send(row, l_nev, MPI_DOUBLE_COMPLEX, dst, 0, mpi_comm_rows, mpierr)
                 endif
               enddo
             enddo
@@ -3445,14 +3445,14 @@
                 if(src == my_prow) then
                     src_offset = src_offset+1
                     row(:) = q(src_offset, 1:l_nev)
-                    call MPI_Send(row, l_nev, MPI_COMPLEX16, ip, 0, mpi_comm_rows, mpierr)
+                    call MPI_Send(row, l_nev, MPI_DOUBLE_COMPLEX, ip, 0, mpi_comm_rows, mpierr)
                 endif
             enddo
             ! Receive all rows from PE ip
             do i=limits(my_prow)+1,limits(my_prow+1)
                 src = mod((i-1)/nblk, np_rows)
                 if(src == ip) then
-                    call MPI_Recv(row, l_nev, MPI_COMPLEX16, src, 0, mpi_comm_rows, mpi_status, mpierr)
+                    call MPI_Recv(row, l_nev, MPI_DOUBLE_COMPLEX, src, 0, mpi_comm_rows, mpi_status, mpierr)
 !$omp parallel do private(my_thread), schedule(static, 1)
                     do my_thread = 1, max_threads
                         call unpack_row(row,i-limits(my_prow),my_thread)
@@ -3479,7 +3479,7 @@
 
     if(my_prow > 0 .and. l_nev>0) then ! note: row 0 always sends
         do j = 1, min(num_result_buffers, num_result_blocks)
-            call MPI_Irecv(result_buffer(1,1,j), l_nev*nblk, MPI_COMPLEX16, 0, result_recv_tag, &
+            call MPI_Irecv(result_buffer(1,1,j), l_nev*nblk, MPI_DOUBLE_COMPLEX, 0, result_recv_tag, &
                            mpi_comm_rows, result_recv_request(j), mpierr)
         enddo
     endif
@@ -3553,7 +3553,7 @@
             do i = 1, stripe_count
                 csw = min(stripe_width, thread_width-(i-1)*stripe_width) ! "current_stripe_width"
                 b_len = csw*nbw*max_threads
-                call MPI_Irecv(bottom_border_recv_buffer(1,i), b_len, MPI_COMPLEX16, my_prow+1, bottom_recv_tag, &
+                call MPI_Irecv(bottom_border_recv_buffer(1,i), b_len, MPI_DOUBLE_COMPLEX, my_prow+1, bottom_recv_tag, &
                            mpi_comm_rows, bottom_recv_request(i), mpierr)
             enddo
         endif
@@ -3563,7 +3563,7 @@
                 bcast_buffer(:,1:current_local_n) = hh_trans_complex(:,current_tv_off+1:current_tv_off+current_local_n)
                 current_tv_off = current_tv_off + current_local_n
             endif
-            call mpi_bcast(bcast_buffer, nbw*current_local_n, MPI_COMPLEX16, mod(sweep,np_cols), mpi_comm_cols, mpierr)
+            call mpi_bcast(bcast_buffer, nbw*current_local_n, MPI_DOUBLE_COMPLEX, mod(sweep,np_cols), mpi_comm_cols, mpierr)
         else
             ! for current_local_n == 1 the one and only HH vector is 0 and not stored in hh_trans_complex
             bcast_buffer(:,1) = 0
@@ -3595,7 +3595,7 @@
                 enddo
                 if(next_n_end < next_n) then
                     call MPI_Irecv(bottom_border_recv_buffer(1,i), csw*nbw*max_threads, &
-                                   MPI_COMPLEX16, my_prow+1, bottom_recv_tag, &
+                                   MPI_DOUBLE_COMPLEX, my_prow+1, bottom_recv_tag, &
                                    mpi_comm_rows, bottom_recv_request(i), mpierr)
                 endif
             endif
@@ -3626,7 +3626,7 @@
                     b_len = csw*bottom_msg_length*max_threads
                     bottom_border_send_buffer(1:b_len,i) = &
                         reshape(a(1:csw,n_off+1:n_off+bottom_msg_length,i,:), (/ b_len /))
-                    call MPI_Isend(bottom_border_send_buffer(1,i), b_len, MPI_COMPLEX16, my_prow+1, &
+                    call MPI_Isend(bottom_border_send_buffer(1,i), b_len, MPI_DOUBLE_COMPLEX, my_prow+1, &
                                    top_recv_tag, mpi_comm_rows, bottom_send_request(i), mpierr)
                 endif
 
@@ -3645,7 +3645,7 @@
                     b_len = csw*bottom_msg_length*max_threads
                     bottom_border_send_buffer(1:b_len,i) = &
                       reshape(a(1:csw,n_off+1:n_off+bottom_msg_length,i,:), (/ b_len /))
-                    call MPI_Isend(bottom_border_send_buffer(1,i), b_len, MPI_COMPLEX16, my_prow+1, &
+                    call MPI_Isend(bottom_border_send_buffer(1,i), b_len, MPI_DOUBLE_COMPLEX, my_prow+1, &
                                    top_recv_tag, mpi_comm_rows, bottom_send_request(i), mpierr)
                 endif
 
@@ -3676,7 +3676,7 @@
             if(next_top_msg_length > 0) then
                 !request top_border data
                 b_len = csw*next_top_msg_length*max_threads
-                call MPI_Irecv(top_border_recv_buffer(1,i), b_len, MPI_COMPLEX16, my_prow-1, &
+                call MPI_Irecv(top_border_recv_buffer(1,i), b_len, MPI_DOUBLE_COMPLEX, my_prow-1, &
                                top_recv_tag, mpi_comm_rows, top_recv_request(i), mpierr)
             endif
 
@@ -3685,7 +3685,7 @@
                 call MPI_Wait(top_send_request(i), mpi_status, mpierr)
                 b_len = csw*nbw*max_threads
                 top_border_send_buffer(1:b_len,i) = reshape(a(1:csw,a_off+1:a_off+nbw,i,:), (/ b_len /))
-                call MPI_Isend(top_border_send_buffer(1,i), b_len, MPI_COMPLEX16, &
+                call MPI_Isend(top_border_send_buffer(1,i), b_len, MPI_DOUBLE_COMPLEX, &
                                my_prow-1, bottom_recv_tag, &
                                mpi_comm_rows, top_send_request(i), mpierr)
             endif
@@ -3736,7 +3736,7 @@
                     do i = 1, nblk
                         call pack_row(result_buffer(:,i,nbuf),j*nblk+i+a_off)
                     enddo
-                    call MPI_Isend(result_buffer(1,1,nbuf), l_nev*nblk, MPI_COMPLEX16, dst, &
+                    call MPI_Isend(result_buffer(1,1,nbuf), l_nev*nblk, MPI_DOUBLE_COMPLEX, dst, &
                                    result_recv_tag, mpi_comm_rows, result_send_request(nbuf), mpierr)
                 endif
             enddo
@@ -3768,7 +3768,7 @@
 
                 ! Queue result buffer again if there are outstanding blocks left
                 if(j+num_result_buffers < num_result_blocks) &
-                    call MPI_Irecv(result_buffer(1,1,nbuf), l_nev*nblk, MPI_COMPLEX16, 0, result_recv_tag, &
+                    call MPI_Irecv(result_buffer(1,1,nbuf), l_nev*nblk, MPI_DOUBLE_COMPLEX, 0, result_recv_tag, &
                                    mpi_comm_rows, result_recv_request(nbuf), mpierr)
 
             enddo
@@ -4068,7 +4068,7 @@
    if(l_real) then
       call MPI_Alltoallv(r_sbuf,ncnt_s,nstart_s,MPI_REAL8,r_rbuf,ncnt_r,nstart_r,MPI_REAL8,mpi_comm,mpierr)
    else
-      call MPI_Alltoallv(c_sbuf,ncnt_s,nstart_s,MPI_COMPLEX16,c_rbuf,ncnt_r,nstart_r,MPI_COMPLEX16,mpi_comm,mpierr)
+      call MPI_Alltoallv(c_sbuf,ncnt_s,nstart_s,MPI_DOUBLE_COMPLEX,c_rbuf,ncnt_r,nstart_r,MPI_DOUBLE_COMPLEX,mpi_comm,mpierr)
    endif
 
    ! set band from receive buffer
diff -urN elpa_lib-201305.orig/ELPA_development_version/test/test_complex2.f90 elpa_lib-201305/ELPA_development_version/test/test_complex2.f90
--- elpa_lib-201305.orig/ELPA_development_version/test/test_complex2.f90	2013-05-21 14:44:19.000000000 +0900
+++ elpa_lib-201305/ELPA_development_version/test/test_complex2.f90	2013-10-01 15:57:54.000000000 +0900
@@ -58,6 +58,7 @@
 
    implicit none
    include 'mpif.h'
+   integer :: iargc
 
    !-------------------------------------------------------------------------------
    ! Please set system size parameters below!
